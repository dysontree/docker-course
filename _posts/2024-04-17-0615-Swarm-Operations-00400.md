---
layout: article
---

Ок, теперь, когда задачи по обновлению и патчингу выполнены, а окно обслуживания завершилось, мы делаем узел снова доступным при помощи той же команды `docker node update`, но уже с настройкой `active`. 

```
docker  node  update --availability active worker-0
```

Узел снова стал активной частью кластера и готов размещать нагрузку. Здесь помни, что Swarm не выполняет автоматическую ребалансировку рабочей нагрузки. Старые контейнеры, которые работают на других нодах, пока там и останутся. Это как если бы мы добавили в кластер новый рабочий узел. На него будет запланирована лишь новая рабочая нагрузка.

Теперь давай посмотрим, как удалить узел из кластера. Скажем, мы хотим удалить второго воркера. Проблема в том, что на нем исполняются две рабочие нагрузки. Здесь последовательность будет такой: начала сделаем `drain` второму рабочему узлу, что переместит контейнеры на другую ноду, а дальше удалим ноду из кластера.

Итак, мы запускаем команду:

```
docker  node  update --availability drain worker-1
```

После этого нужно перейти на отсоединяемый узел и выполнить команду `docker swarm leave`.

```
docker swarm leave
```

Теперь `worker-1` не фактически является частью кластера. Тем не менее с точки зрения руководства кластера эта нода всё еще на месте, просто недоступна.

```
docker node ls
```

Это произошло потому, что только менеджеры могут менять записи о состоянии кластера. При выходе из Swarm наш рабочий узел удалил своё локальное состояние, которое относилось к кластеру и перестал с ним общаться. Но с точки зрения менеджера узел всё ещё является частью кластера, но просто отлучился на время. На самом деле такое поведение ты можешь увидеть и по другим причинам, когда с нодой что-то произошло, и она отключилась по причинам, требующим нашего вмешательства.

Если же мы уверены, что всё идёт хорошо, нам нужно запустить команду `docker node rm worker-1`, после чего узел полностью пропадёт из списка.

Теперь наш swarm-кластер остался всего с двумя узлами.

И это всё, увидимся!
